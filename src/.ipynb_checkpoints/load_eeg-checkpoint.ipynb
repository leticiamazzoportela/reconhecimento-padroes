{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregamento e preparação de *datasets*\n",
    "\n",
    "Neste `notebook` será apresentado o carregamento de um *dataset* público do *website* `UCI - Machine Learning Repository`. O *dataset* a ser utilizado é o `EEG Database Data Set` (https://archive.ics.uci.edu/ml/datasets/EEG+Database).\n",
    "\n",
    "\n",
    "## Descrição do *dataset*:\n",
    "\n",
    "A intenção deste *dataset* é examinar por meio de algoritmos de inteligência computacional a pré-disposição genética que um paciente possui ao alcoolismo.\n",
    "\n",
    "Os principais dados analisados são do tipo *time-series*, em outras palavras, conjuntos de dados que representam um sinal mensurado no domínio do tempo. Os dados são completados com outros atributos como o nome do eletrodo, o número da amostra, etc. Outras informações relevantes do *dataset*:\n",
    "\n",
    "- Quantidade de atributos: 4\n",
    "- Número de instancias: 122\n",
    "- Existem dados faltantes? Sim\n",
    "- Tipos de dados encontrados: categórico, inteiro e real\n",
    "\n",
    "Existem três categorias de dados neste *dataset*:\n",
    "\n",
    "1. Small Data Set: Contém dados para 2 tipos de indivíduos: alcoólico e controle. São exibidas 10 execuções para cada um dos paradigmas a_1 e c_1, a_m e c_m, a_n e c_n.\n",
    "2. Large Data Set: Contém dados para 10 indivíduos alcoólicos e 10 de controle. São exibidas, também, 10 execuções para cada um dos paradigmas. Os dados do teste usaram os mesmos 10 indivíduos alcoólicos e 10 controles, assim como os dados de treinamento, mas com 10 execuções fora da amostra por paradigma. \n",
    "3. Full Data Set: Contém todos os 120 testes para 122 indivíduos.\n",
    "\n",
    "Cada sessão (*trial*) é armazenada da seguinte forma:\n",
    "\n",
    "```\n",
    "# co2a0000364.rd \n",
    "# 120 trials, 64 chans, 416 samples 368 post_stim samples \n",
    "# 3.906000 msecs uV \n",
    "# S1 obj , trial 0 \n",
    "# FP1 chan 0 \n",
    "0 FP1 0 -8.921 \n",
    "0 FP1 1 -8.433 \n",
    "0 FP1 2 -2.574 \n",
    "0 FP1 3 5.239 \n",
    "0 FP1 4 11.587 \n",
    "0 FP1 5 14.028\n",
    "...\n",
    "```\n",
    "\n",
    "As primeiras 4 linhas são de cabeçalho:\n",
    "\n",
    "**linha 1**: identificação do paciente e se ele indica ser um alcoólatra (a) ou controle (c) pela quarta letra (co2**a**0000364);\n",
    "\n",
    "**linha 4**: determina se o paciente foi exposto a um único estímulo (`S1 obj`), a dois estímulos iguais (`S2 match`) ou a dois estímulos diferentes (`S2 no match`);\n",
    "\n",
    "**linha 5**: identifica o início da coleta dos dados pelo eletrodo FP1. As 4 colunas são:\n",
    "\n",
    "```\n",
    "número_da_sessão identificação_do_eletrodo número_da_amostra valor_em_micro_volts\n",
    "```\n",
    "\n",
    "\n",
    "### Realizando o download \n",
    "\n",
    "Primeiramente, faremos um código para verificar se o *dataset* já foi baixado, caso contrário, executar o código de download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset já baixado!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "import os\n",
    "\n",
    "\n",
    "urls = {\n",
    "    'small': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/smni_eeg_data.tar.gz',\n",
    "    'large_train': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/SMNI_CMI_TRAIN.tar.gz',\n",
    "    'large_test': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/SMNI_CMI_TEST.tar.gz',\n",
    "    'full': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/eeg_full.tar'\n",
    "}\n",
    "\n",
    "# verifica se o diretório dos datasets existe\n",
    "if not os.path.exists('dataset/'):\n",
    "    os.mkdir('dataset/')\n",
    "    for k, v in urls.items():\n",
    "        fn = v.split('/')[-1]\n",
    "        print('Baixando:', fn, '...')\n",
    "        urlretrieve(v, './dataset/{}'.format(fn))\n",
    "    print('Downlod dos datasets concluído!')\n",
    "else:\n",
    "    print('Dataset já baixado!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descompactando pastas e subpastas\n",
    "\n",
    "Agora é necessário descompactar (recursivamente) diversas pastas e subpastas em arquivos GZip. Algumas pastas estão com o arquivo na extensão `.tar`, já outras, `.tar.gz`. Não obstante, algumas subpastas estão compactadas e outras não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'dataset/eeg_full/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d451d2e640dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# único arquivo somente empacotado (tar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/eeg_full/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tar -xvf dataset/eeg_full.tar -C dataset/eeg_full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/eeg_full.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'dataset/eeg_full/'"
     ]
    }
   ],
   "source": [
    "from subprocess import getoutput as gop\n",
    "import glob\n",
    "\n",
    "\n",
    "# único arquivo somente empacotado (tar)\n",
    "os.mkdir('dataset/eeg_full/')\n",
    "gop('tar -xvf dataset/eeg_full.tar -C dataset/eeg_full')\n",
    "os.remove('dataset/eeg_full.tar')\n",
    "\n",
    "while glob.glob('dataset/**/*.gz', recursive=True):\n",
    "    # quando o arquivo está empacotado (tar) e compactado (gz)\n",
    "    for f in glob.iglob('dataset/**/*.tar.gz', recursive=True):\n",
    "        gop('tar -zxvf {} -C {}'.format(f, f[:f.rindex('/')]))\n",
    "        os.remove(f)\n",
    "    # quando o arquivo está somente compactado (gz)\n",
    "    for f in glob.iglob('dataset/**/*.gz', recursive=True):\n",
    "        gop('gzip -d {}'.format(f))\n",
    "print('Descompactações finalizadas!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/smni_eeg_data' -> 'dataset/small'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1f6f56966389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# organizando melhor as pastas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/smni_eeg_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset/small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/eeg_full'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset/full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/SMNI_CMI_TRAIN/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset/large_train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/smni_eeg_data' -> 'dataset/small'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# organizando melhor as pastas\n",
    "os.rename('dataset/smni_eeg_data', 'dataset/small')\n",
    "os.rename('dataset/eeg_full', 'dataset/full')\n",
    "os.rename('dataset/SMNI_CMI_TRAIN/', 'dataset/large_train/')\n",
    "os.rename('dataset/SMNI_CMI_TEST/', 'dataset/large_test/')\n",
    "print(gop('ls -l dataset/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para carregar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "import numpy as np\n",
    "from subprocess import getoutput as gop\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# identificando pastas\n",
    "folders = {\n",
    "    'small': 'dataset/small',\n",
    "    'large_train': 'dataset/large_train',\n",
    "    'large_test': 'dataset/large_test',\n",
    "    'full': 'dataset/full',\n",
    "}\n",
    "\n",
    "def load_data(dataset):\n",
    "    ch_names = []\n",
    "    create_ch_name = False\n",
    "    \n",
    "    data_dir = gop('ls {}'.format(folders[dataset])).split('\\n')\n",
    "    # 1ª dimensão dos dados contendo os sujeitos\n",
    "    subjects = list()\n",
    "    \n",
    "    for types in data_dir:\n",
    "        files = gop('ls {}/{}'.format(folders[dataset], types)).split('\\n')\n",
    "        # 2ª dimensão dos dados contendo as sessões (trials)\n",
    "        trials = list()\n",
    "        \n",
    "        for f in files:\n",
    "            arquivo = open('{}/{}/{}'.format(folders[dataset], types, f))\n",
    "            text = arquivo.readlines()\n",
    "            \n",
    "            # 3ª dimensão dos dados contendo os canais (eletrodos)\n",
    "            chs = list()\n",
    "            # 4ª dimensão dos dados contendo os valores em milivolts\n",
    "            values = list()\n",
    "            for line in text:\n",
    "                t = search('(?P<ch_name>\\w{1,3}) chan \\d{1,2}', line)\n",
    "                p = search('^\\d{1,3}\\ \\w{1,3}\\ \\d{1,3}\\ (?P<value>.+$)', line)\n",
    "                                    \n",
    "                if p:\n",
    "                    values.append(float(p.group('value')))\n",
    "                # mudou para outro eletrodo\n",
    "                elif t:\n",
    "                    if values:\n",
    "                        chs.append(values)\n",
    "                        values = list()\n",
    "                    if not create_ch_name:\n",
    "                        ch_names.append(t.group('ch_name').lower())\n",
    "            \n",
    "            create_ch_name = True\n",
    "            chs.append(values)\n",
    "            trials.append(chs)\n",
    "            md = np.average(trials, axis=0)\n",
    "            arquivo.close()\n",
    "            \n",
    "        subjects.append(trials)\n",
    "        subjects.append('\\n')\n",
    "            \n",
    "    data = np.array(subjects)\n",
    "            \n",
    "    return data, md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exibindo resultado do carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small:  (12,)\n",
      "Média:  [[ -2.7395  -3.9114  -4.6438 ...  -0.3467  -1.1768  -1.1283]\n",
      " [ -3.3102  -4.3844  -4.8239 ...   3.5259   2.1586   0.9379]\n",
      " [ -6.0904  -5.7485  -4.4792 ... -12.3891 -10.973   -8.6783]\n",
      " ...\n",
      " [ -0.2107  -0.0639   0.2776 ...  -2.7008  -2.5056  -2.5544]\n",
      " [ -2.8331  -3.8096  -4.835  ...  -0.4403  -1.1241  -1.2218]\n",
      " [ -4.7454  -5.7219  -6.4055 ... -12.6068 -13.2903 -13.6811]]\n",
      "Shape da Média:  (64, 256)\n",
      "*********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_data, md_small = load_data('small')\n",
    "print('Small: ', small_data.shape)\n",
    "print('Média: ', md_small)\n",
    "print('Shape da Média: ', md_small.shape)\n",
    "print('*********************\\n')\n",
    "\n",
    "large_train_data, md_train = load_data('large_train')\n",
    "print('Large Train: ', large_train_data.shape)\n",
    "print('Média: ', md_train)\n",
    "print('Shape da Média: ', md_train.shape)\n",
    "print('*********************\\n')\n",
    "\n",
    "large_test_data, md_test = load_data('large_test')\n",
    "print('Large Test: ', large_test_data.shape)\n",
    "print('Média: ', md_test)\n",
    "print('Shape da Média: ', md_test.shape)\n",
    "print('*********************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados carregados...\n",
    "\n",
    "Os dados \"single\" foram dividos da seguinte forma:\n",
    "```\n",
    "[experimentos, triagens, canais, amostras]\n",
    "```\n",
    "formando um `numpy.array` de quatro dimensões.\n",
    "\n",
    "Em seguida, vamos plotar esses dados para \"tentar\" visualizar algum padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data):\n",
    "    \n",
    "    %matplotlib inline\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib import cm\n",
    "\n",
    "    d1 = list()\n",
    "    d2 = list()\n",
    "\n",
    "    for e in range(64):\n",
    "        for i, t in enumerate(np.linspace(0, 1, 256)):\n",
    "            d1.append([e, t, data[0][0][e][i]])\n",
    "            d2.append([e, t, data[1][0][e][i]])\n",
    "    d1 = np.array(d1)\n",
    "    d2 = np.array(d2)\n",
    "    x1, y1, z1 = d1[:,0], d1[:,1], d1[:,2]\n",
    "    x2, y2, z2 = d2[:,0], d2[:,1], d2[:,2]\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    surf = ax.plot_trisurf(x1, y1, z1, cmap=cm.inferno, linewidth=1)\n",
    "    ax.set_xlabel('Canais')\n",
    "    ax.set_ylabel('Tempo (seg.)')\n",
    "    ax.set_zlabel('Milivolts')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    surf = ax.plot_trisurf(x2, y2, z2, cmap=cm.inferno, linewidth=1)\n",
    "    ax.set_xlabel('Canais')\n",
    "    ax.set_ylabel('Tempo (seg.)')\n",
    "    ax.set_zlabel('Milivolts')\n",
    "\n",
    "    fig.colorbar(surf)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_average(data):\n",
    "    \n",
    "    %matplotlib inline\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib import cm\n",
    "\n",
    "    d1 = list()\n",
    "    d2 = list()\n",
    "\n",
    "    for e in range(64):\n",
    "        for i, t in enumerate(np.linspace(0, 1, 256)):\n",
    "            d1.append([e, t, data[e][i]])\n",
    "            d2.append([e, t, data[e][i]])\n",
    "    d1 = np.array(d1)\n",
    "    d2 = np.array(d2)\n",
    "    x1, y1, z1 = d1[:,0], d1[:,1], d1[:,2]\n",
    "    x2, y2, z2 = d2[:,0], d2[:,1], d2[:,2]\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    surf = ax.plot_trisurf(x1, y1, z1, cmap=cm.inferno, linewidth=1)\n",
    "    ax.set_xlabel('Canais')\n",
    "    ax.set_ylabel('Tempo (seg.)')\n",
    "    ax.set_zlabel('Milivolts')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    surf = ax.plot_trisurf(x2, y2, z2, cmap=cm.inferno, linewidth=1)\n",
    "    ax.set_xlabel('Canais')\n",
    "    ax.set_ylabel('Tempo (seg.)')\n",
    "    ax.set_zlabel('Milivolts')\n",
    "\n",
    "    fig.colorbar(surf)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Small')\n",
    "show_data(small_data)\n",
    "\n",
    "print('Média Small')\n",
    "show_average(md_small)\n",
    "print('*****************\\n')\n",
    "\n",
    "print('Large Train')\n",
    "show_data(large_train_data)\n",
    "\n",
    "print('Média Large Train')\n",
    "show_average(md_train)\n",
    "print('*****************\\n')\n",
    "\n",
    "print('Large Test')\n",
    "show_data(large_test_data)\n",
    "\n",
    "print('Média Large Test')\n",
    "show_average(md_test)\n",
    "print('*****************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
