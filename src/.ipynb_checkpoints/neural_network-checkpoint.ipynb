{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando bibliotecas necessárias no projeto\n",
    "from sklearn import svm\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.svm import SVC\n",
    "from keras import utils as np_utils\n",
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from re import search\n",
    "import numpy as np\n",
    "from subprocess import getoutput as gop\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando conjuntos de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'dataset/large_train/README/dataset/large_train/README'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2588a2443b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_alc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_alc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ctrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_ctrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mdata_alc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_alc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ctrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_ctrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'large_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'**** Large Train ****\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-2588a2443b38>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0marquivo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marquivo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'dataset/large_train/README/dataset/large_train/README'"
     ]
    }
   ],
   "source": [
    "# identificando pastas\n",
    "folders = {\n",
    "    'small': 'dataset/small',\n",
    "    'large_train': 'dataset/large_train',\n",
    "    'large_test': 'dataset/large_test',\n",
    "}\n",
    "\n",
    "def load_data(dataset):\n",
    "    ch_names = []\n",
    "    create_ch_name = False\n",
    "    \n",
    "    data_dir = gop('ls {}'.format(folders[dataset])).split('\\n')\n",
    "    # 1ª dimensão dos dados contendo os sujeitos\n",
    "    subjects = list()\n",
    "    subjects_alc = list()\n",
    "    subjects_ctrl = list()\n",
    "    \n",
    "    for types in data_dir:\n",
    "        files = gop('ls {}/{}'.format(folders[dataset], types)).split('\\n')\n",
    "        # 2ª dimensão dos dados contendo as sessões (trials)\n",
    "        trials = list()\n",
    "        is_alc = True\n",
    "        \n",
    "        for f in files:\n",
    "            arquivo = open('{}/{}/{}'.format(folders[dataset], types, f))\n",
    "            text = arquivo.readlines()\n",
    "            \n",
    "            alc = search('co2a', text[0])\n",
    "            ctrl = search('co2c', text[0])\n",
    "            \n",
    "            if ctrl:\n",
    "                is_alc = False\n",
    "            # 3ª dimensão dos dados contendo os canais (eletrodos)\n",
    "            chs = list()\n",
    "            # 4ª dimensão dos dados contendo os valores em milivolts\n",
    "            values = list()\n",
    "            for line in text:\n",
    "                t = search('(?P<ch_name>\\w{1,3}) chan \\d{1,2}', line)\n",
    "                p = search('^\\d{1,3}\\ \\w{1,3}\\ \\d{1,3}\\ (?P<value>.+$)', line)\n",
    "                                    \n",
    "                if p:\n",
    "                    values.append(float(p.group('value')))\n",
    "                # mudou para outro eletrodo\n",
    "                elif t:\n",
    "                    if values:\n",
    "                        chs.append(values)\n",
    "                        values = list()\n",
    "                    if not create_ch_name:\n",
    "                        ch_names.append(t.group('ch_name').lower())\n",
    "            \n",
    "            create_ch_name = True\n",
    "            chs.append(values)\n",
    "            trials.append(chs)\n",
    "            arquivo.close()\n",
    "            \n",
    "        if is_alc:\n",
    "            subjects_alc.append(trials)\n",
    "            md_alc = np.average(trials, axis=0)\n",
    "        else:\n",
    "            subjects_ctrl.append(trials)\n",
    "            md_ctrl = np.average(trials, axis=0)\n",
    "            \n",
    "    data_alc = np.array(subjects_alc)\n",
    "    data_ctrl = np.array(subjects_ctrl)\n",
    "    \n",
    "#     data.tofile('./dataset_csv/small_data.csv', sep=',', newline='')   \n",
    "    return data_alc, md_alc, data_ctrl, md_ctrl, ch_names\n",
    "\n",
    "data_alc, md_alc, data_ctrl, md_ctrl, ch_names = load_data('large_train')\n",
    "\n",
    "print('**** Large Train ****\\n')\n",
    "print('Alcoólatras: ', data_alc)\n",
    "print('\\n**** Fim Large Train ****\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_train_alc, md_alc_train, lg_train_ctrl, md_ctrl_train, ch_names = load_data('large_train')\n",
    "lg_test_alc, md_alc_test, lg_test_ctrl, md_ctrl_test, ch_names = load_data('large_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = Sequential()\n",
    "num_input = lg_train_alc.shape[1]\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=num_input))\n",
    "model.add(Dense(units=59, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(md_alc_train, md_ctrl_train, validation_split=0.30, epochs=50, batch_size=50, verbose=1)\n",
    "\n",
    "loss_and_metrics = model.evaluate(md_alc_test, md_ctrl_test, batch_size=50)\n",
    "print(\"\\n Taxa de acerto: %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Função de Custo: %.2f%%\" % (loss_and_metrics[0]))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
