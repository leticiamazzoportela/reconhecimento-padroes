{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando bibliotecas necessárias no projeto\n",
    "from sklearn import svm\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.svm import SVC\n",
    "from keras import utils as np_utils\n",
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from re import search\n",
    "import numpy as np\n",
    "from subprocess import getoutput as gop\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando conjuntos de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identificando pastas\n",
    "folders = {\n",
    "    'small': 'dataset/small',\n",
    "    'large_train': 'dataset/large_train',\n",
    "    'large_test': 'dataset/large_test',\n",
    "}\n",
    "\n",
    "def load_data(dataset):\n",
    "    ch_names = []\n",
    "    create_ch_name = False\n",
    "    \n",
    "    data_dir = gop('ls {}'.format(folders[dataset])).split('\\n')\n",
    "    # 1ª dimensão dos dados contendo os sujeitos\n",
    "    subjects = list()\n",
    "    subjects_alc = list()\n",
    "    subjects_ctrl = list()\n",
    "    \n",
    "    for types in data_dir:\n",
    "        files = gop('ls {}/{}'.format(folders[dataset], types)).split('\\n')\n",
    "        # 2ª dimensão dos dados contendo as sessões (trials)\n",
    "        trials = list()\n",
    "        is_alc = True\n",
    "        \n",
    "        for f in files:\n",
    "            arquivo = open('{}/{}/{}'.format(folders[dataset], types, f))\n",
    "            text = arquivo.readlines()\n",
    "            \n",
    "            alc = search('co2a', text[0])\n",
    "            ctrl = search('co2c', text[0])\n",
    "            \n",
    "            if ctrl:\n",
    "                is_alc = False\n",
    "            # 3ª dimensão dos dados contendo os canais (eletrodos)\n",
    "            chs = list()\n",
    "            # 4ª dimensão dos dados contendo os valores em milivolts\n",
    "            values = list()\n",
    "            for line in text:\n",
    "                t = search('(?P<ch_name>\\w{1,3}) chan \\d{1,2}', line)\n",
    "                p = search('^\\d{1,3}\\ \\w{1,3}\\ \\d{1,3}\\ (?P<value>.+$)', line)\n",
    "                                    \n",
    "                if p:\n",
    "                    values.append(float(p.group('value')))\n",
    "                # mudou para outro eletrodo\n",
    "                elif t:\n",
    "                    if values:\n",
    "                        chs.append(values)\n",
    "                        values = list()\n",
    "                    if not create_ch_name:\n",
    "                        ch_names.append(t.group('ch_name').lower())\n",
    "            \n",
    "            create_ch_name = True\n",
    "            chs.append(values)\n",
    "            trials.append(chs)\n",
    "            arquivo.close()\n",
    "            \n",
    "        if is_alc:\n",
    "            subjects_alc.append(trials)\n",
    "            md_alc = np.average(trials, axis=0)\n",
    "        else:\n",
    "            subjects_ctrl.append(trials)\n",
    "            md_ctrl = np.average(trials, axis=0)\n",
    "            \n",
    "    data_alc = np.array(subjects_alc)\n",
    "    data_ctrl = np.array(subjects_ctrl)\n",
    "    \n",
    "#     data.tofile('./dataset_csv/small_data.csv', sep=',', newline='')   \n",
    "    return data_alc, md_alc, data_ctrl, md_ctrl, ch_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alc, md_alc, data_ctrl, md_ctrl, ch_names = load_data('large_train')\n",
    "data_alc_test, md_alc_test, data_ctrl_test, md_ctrl_test, ch_names_test = load_data('large_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treino\n",
    "\n",
    "y_md_alc = []\n",
    "count = 0\n",
    "for i in range(len(md_alc)):\n",
    "    y_md_alc.insert(count, True)\n",
    "    count += 1\n",
    "\n",
    "y_md_ctrl = []\n",
    "count = 0\n",
    "for i in range(len(md_ctrl)):\n",
    "    y_md_ctrl.insert(count, False)\n",
    "    count += 1\n",
    "\n",
    "md_total = np.concatenate((md_alc, md_ctrl))\n",
    "y_md_total = np.concatenate((y_md_alc, y_md_ctrl))\n",
    "\n",
    "#Teste\n",
    "\n",
    "y_md_alc_test = []\n",
    "count = 0\n",
    "for i in range(len(md_alc_test)):\n",
    "    y_md_alc_test.insert(count, True)\n",
    "    count += 1\n",
    "\n",
    "y_md_ctrl_test = []\n",
    "count = 0\n",
    "for i in range(len(md_ctrl_test)):\n",
    "    y_md_ctrl_test.insert(count, False)\n",
    "    count += 1\n",
    "    \n",
    "md_total_test = np.concatenate((md_alc_test, md_ctrl_test))\n",
    "y_md_total_test = np.concatenate((y_md_alc_test, y_md_ctrl_test))\n",
    "\n",
    "\n",
    "md_array = []\n",
    "md_total_com_y = []\n",
    "count = 0\n",
    "count_md = 0\n",
    "i = 0\n",
    "for item in md_total:\n",
    "    for subitem in item:\n",
    "        md_total_com_y.insert(count, subitem)\n",
    "        count += 1\n",
    "    md_total_com_y.insert(count, y_md_total[i])\n",
    "    count = 0\n",
    "    md_array.insert(count_md, md_total_com_y)\n",
    "    md_total_com_y = []\n",
    "    count_md += 1\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "test_array = []\n",
    "test_total_com_y = []\n",
    "count = 0\n",
    "count_test = 0\n",
    "i = 0\n",
    "for item in md_total_test:\n",
    "    for subitem in item:\n",
    "        test_total_com_y.insert(count, subitem)\n",
    "        count += 1\n",
    "    test_total_com_y.insert(count, y_md_total_test[i])\n",
    "    count = 0\n",
    "    test_array.insert(count_test, test_total_com_y)\n",
    "    test_total_com_y = []\n",
    "    count_test += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "[False, False, True, True, True, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, False, False, True, False, False, False, True, True, False, True, False, False, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, False, True, False, True, False, True, True, True, False, True, True, False, True, False, False, True, True, False, True, False, False, False, True, True, False, False, True, False, True, True, False, True, True, True, False, False, True, True, False, True, False, False, True, False, False, False, True, True, False, False, False, True, False, True, False, False, True, False, False, True, True, False, False, True, True, True, True, True, True, True, False]\n",
      "256\n",
      "[True, False, False, False, False, True, False, False, False, True, True, False, True, True, True, True, False, False, True, False, False, True, True, True, False, False, False, True, True, True, False, False, False, False, True, False, True, True, False, True, False, True, False, True, False, False, False, True, True, True, True, True, True, False, False, True, False, True, False, True, False, False, True, True, False, False, True, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, False, True, False, False, False, True, True, False, False, False, False, True, False, True, False, True, True, True, True, True, True, True, True, False, True, False, False, True, True, False, False, False, True, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(md_array)\n",
    "random.shuffle(test_array)\n",
    "\n",
    "index = 0\n",
    "y_train = []\n",
    "for i in range(len(md_array)):\n",
    "    result = md_array[i][256] #Vetor de resultados\n",
    "    y_train.insert(index, result)\n",
    "    index += 1\n",
    "\n",
    "for i in range(len(md_array)):\n",
    "    del(md_array[i][256])\n",
    "\n",
    "x_train = md_array\n",
    "print(len(md_array[0]))\n",
    "print(y_train)\n",
    "\n",
    "index = 0\n",
    "y_test = []\n",
    "for i in range(len(test_array)):\n",
    "    result = test_array[i][256] #Vetor de resultados\n",
    "    y_test.insert(index, result)\n",
    "    index += 1\n",
    "\n",
    "for i in range(len(test_array)):\n",
    "    del(test_array[i][256])\n",
    "\n",
    "x_test = test_array\n",
    "print(len(test_array[0]))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 128 arrays: [array([[-0.2773    ],\n       [ 0.0156    ],\n       [-0.1471    ],\n       [-0.53776667],\n       [-0.8308    ],\n       [-1.04236667],\n       [-1.04236667],\n       [-1.15626667],\n       [-1.2702    ],\n ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-234-d9c92e1dd767>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 128 arrays: [array([[-0.2773    ],\n       [ 0.0156    ],\n       [-0.1471    ],\n       [-0.53776667],\n       [-0.8308    ],\n       [-1.04236667],\n       [-1.04236667],\n       [-1.15626667],\n       [-1.2702    ],\n ..."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=256))\n",
    "model.add(Dense(units=59, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.30, epochs=30, batch_size=16, verbose=1)\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=16)\n",
    "print(\"\\n Taxa de acerto: %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Função de Custo: %.2f%%\" % (loss_and_metrics[0]))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
